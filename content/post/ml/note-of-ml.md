## 一. 引言 ##

### 1.1 机器学习 ###

Tom Mitchell （CMU）一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升  。

分为监督学习和无监督学习两类。监督学习尝试教机器如何完成任务。无监督学习是让机器自己进行学习 。

### 1.2 监督学习 ###

监督学习指的就是我们给学习算法一个数据集，通过数据集（历史样本），预测新的数值（未来）。

#### 1.2.1 逻辑回归 ####

预测房价 ，根据历史，直观上（几何）寻找一条直线或曲线，推测一个连续的结果。

回归这个词的意思是，我们在试着推测出这一系列连续值属性。  

#### 1.2.2 分类 ####

分类指的是，我们试着推测出离散的输出值 。比如通过肿瘤的尺寸（特征），推测是哪种类型（离散值）的肿瘤。

垃圾邮件的分类可以看做是二值的。

### 1.3 无监督学习 ###

无监督学习的样本中没有任何的标签信息，但无监督学习算法（聚类算法）可能会把这些数据分成两个不同的簇。

一个具体的应用是新闻聚类。

## 二、单变量线性回归  ##

### 2.1  模型表示 ###

![image-20200520161034755](E:\WorkSpace\github\ZenBlog\static\post\windows\note-of-ml\image-20200520161034755.png)


$$
ℎ𝜃(𝑥) = 𝜃_0 + 𝜃_1𝑥
$$
ℎ 根据输入的 𝑥值来得出 𝑦 值， 𝑦 值对应房子的价格 因此， ℎ 是一个从𝑥
到 𝑦 的函数映射。 

如上，只含有一个特征/输入变量的问题叫作单变量线性回归问题。

### 2.2 代价函数 ###

#### 2.2.1 误差函数 ####

代价函数也被称作平方误差函数，有时也被称为平方误差代价函数，在三维空间中


$$
𝐽(𝜃_0, 𝜃_1) = \dfrac {1}{2𝑚}\sum_{i=1}^{m}(ℎ_𝜃(𝑥_𝑖) -  𝑦_𝑖))^2
$$

#### 2.2.2 推导过程 ####



#### 2.2.3 图像含义 ####



### 2.3 梯度下降 ###

如上的求解过程，可以抽象为


$$
θ=θ−η⋅∇_θJ_{(θ)}
$$


## 三、 线性代数回顾 ##

### 3.1 矩阵和向量 ###

矩阵也称为行列式，其表示一般为
$$
A = \begin{vmatrix}
{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\
{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\
{\vdots}&{\vdots}&{\ddots}&{\vdots}\\
a_{m1}&a_{m2}&{\dots}&a_{mn}\end{vmatrix}
$$


矩阵的维数即行数(m)×列数(n)，𝐴𝑖𝑗指第𝑖行，第𝑗列的元素 。

向量是一种特殊的矩阵，  一般是列向量，如下列y 向量为四维列向量（ 4×1）  
$$
y = \begin{Bmatrix}
{460 }\\
{232 }\\
{315 }\\
{178}\end{Bmatrix}
$$






### 3.2 矩阵加法 ###

相同形状的两个矩阵，对应下表的元素相加



### 3.3 矩阵乘法 ###

𝑚 × 𝑛矩阵乘以𝑛 × 𝑜矩阵，变成𝑚 × 𝑜矩阵
$$
\begin{cases}
a_1x+b_1y+{\dots} + c_1z=d_1\\
a_2x+b_2y+c_2z=d_2\\
a_ix+biy{\vdots}&{\vdots} \\
a_nx+b_ny+c_nz=d_n
\end{cases}
$$



#### 3.3.1 含义 ####

乘法相当于

#### 3.3.2 性质 ####

不满足交换律
$$
A \times B \not\neq B \times A \\
A \times (B \times C) = (A \times B) \times C \\
A \times A^{-1} = A^{-1} \times A = I \\
I \times A = A \times I = A
$$
转置的性质
$$
(A \pm B)^T = A^T \pm B^T       \\
(A \times B)^T = B^T \times A^T \\
(A^T)^T = A                     \\
(KA)^T = KA^T
$$
如何证明一下呢？



## 四. 多变量线性回归 ##

在房价预测模型中，增加多个变量（特征），如楼层，装修年限等，组成向量
$$
(𝑥_1, 𝑥_2, ..., 𝑥_𝑛)
$$
线性假设的表达式为
$$
ℎ_𝜃(𝑥) = 𝜃_0 + 𝜃_1𝑥_1 + 𝜃_2𝑥_2+. . . +𝜃_𝑛𝑥_𝑛
$$
表达式中，有𝑛 + 1个参数和𝑛个变量，为了使得公式能够简化一些，引入
$$
𝑥_0 = 1
$$
则上述表达式可以写为
$$
ℎ_𝜃(𝑥) = 𝜃_0x_0 + 𝜃_1𝑥_1 + 𝜃_2𝑥_2+. . . +𝜃_𝑛𝑥_𝑛
$$
模型中的参数是一个𝑛 + 1维的向量，用矩阵形式简化为
$$
ℎ_𝜃(𝑥) = 𝜃^𝑇𝑋
$$
  可以推导其代价函数为
$$
𝐽(𝜃_0,𝜃_1, ... 𝜃_n) = \dfrac {1}{2𝑚}\sum_{i=1}^{m}(ℎ_𝜃(𝑥^{(i)}) − 𝑦^{(𝑖)}))^2
$$
求导可以得到，当 n >=1 时，各个参数的解为
$$
𝜃_0 := 𝜃_0 − 𝑎\dfrac {1}{𝑚}\sum_{i=1}^{m}(ℎ_𝜃(𝑥^{(i)}) − 𝑦^{(𝑖)})𝑥_0^{(𝑖)}\\
𝜃_1 := 𝜃_1 − 𝑎\dfrac {1}{𝑚}\sum_{i=1}^{m}(ℎ_𝜃(𝑥^{(i)}) − 𝑦^{(𝑖)})𝑥_1^{(𝑖)}\\
{\cdots} \\
𝜃_n := 𝜃_n − 𝑎\dfrac {1}{𝑚}\sum_{i=1}^{m}(ℎ_𝜃(𝑥^{(i)}) − 𝑦^{(𝑖)})𝑥_n^{(𝑖)}
$$
其中 a 为初始化参数。

用计算机实现为



## 五、Python的绘图 ##



## 六、逻辑回归 ##

用逻辑回归解决分类问题。

### 6.1 二元分类 ###

将因变量(dependent variable)可能属于的两个类分别称为负向类（negative class）和正向类（positive class），则因变量 y 的值域属于 0,1，其中 0 表示负向类，1 表示正向类。

逻辑回归算法，它的输出值永远在 0 到 1 之间。



### 6.2 假设表示 ###

S 形函数（Sigmoid function）
$$
G(z) = \dfrac {1}{1+e^{-z}}
$$

### 6.3 代价函数 ###



## 七、正则化 ##

## 八、神经网络 ##

### 8.1 表述 ###

### 8.2 学习 ###

## 机器学习系统的设计 ##





## 支持向量机 ##



## 聚类 ##



## 降维 ##

/data/nps_convert/banner_sample//

## 异常检测 ##



## 推荐系统 ##

### 原因 ###

* 业界热、学界冷
* 对机器学习来说，特征是很重要的，你所选择的特征，将对你学习算法的性能有很大的影响  

### 基于内容的推荐系统 ###

###   协同过滤   ###









